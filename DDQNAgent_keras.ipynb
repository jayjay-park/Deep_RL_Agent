{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    \n",
    "    def __init__(self, env, network, buffer, epsilon=0.05, batch_size=32):\n",
    "        \n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.target_network = deepcopy(network)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.window = 100\n",
    "        self.reward_threshold = 195 # Avg reward before CartPole is \"solved\"\n",
    "        self.initialize()\n",
    "    \n",
    "    def take_step(self, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = self.network.get_action(self.s_0, epsilon=self.epsilon)\n",
    "            self.step_count += 1\n",
    "        s_1, r, done, _ = self.env.step(action)\n",
    "        self.rewards += r\n",
    "        self.buffer.append(self.s_0, action, r, done, s_1)\n",
    "        self.s_0 = s_1.copy()\n",
    "        if done:\n",
    "            self.s_0 = env.reset()\n",
    "        return done\n",
    "        \n",
    "    # Implement DQN training algorithm\n",
    "    def train(self, gamma=0.99, max_episodes=10000, \n",
    "              batch_size=32,\n",
    "              network_update_frequency=4,\n",
    "              network_sync_frequency=2000):\n",
    "        self.gamma = gamma\n",
    "        # Populate replay buffer\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(mode='explore')\n",
    "            \n",
    "        ep = 0\n",
    "        training = True\n",
    "        while training:\n",
    "            self.s_0 = self.env.reset()\n",
    "            self.rewards = 0\n",
    "            done = False\n",
    "            while done == False:\n",
    "                done = self.take_step(mode='train')\n",
    "                # Update network\n",
    "                if self.step_count % network_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Sync networks\n",
    "                if self.step_count % network_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.network.state_dict())\n",
    "                    self.sync_eps.append(ep)\n",
    "                    \n",
    "                if done:\n",
    "                    ep += 1\n",
    "                    self.training_rewards.append(self.rewards)\n",
    "                    self.training_loss.append(np.mean(self.update_loss))\n",
    "                    self.update_loss = []\n",
    "                    mean_rewards = np.mean(\n",
    "                        self.training_rewards[-self.window:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                        ep, mean_rewards), end=\"\")\n",
    "                    \n",
    "                    if ep >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            ep))\n",
    "                        break\n",
    "                        \n",
    "    def calculate_loss(self, batch):\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_t = torch.FloatTensor(rewards).to(device=self.network.device).reshape(-1,1)\n",
    "        actions_t = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.network.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.network.device)\n",
    "        \n",
    "        qvals = torch.gather(self.network.get_qvals(states), 1, actions_t)\n",
    "        \n",
    "        #################################################################\n",
    "        # DDQN Update\n",
    "        next_actions = torch.max(self.network.get_qvals(next_states), dim=-1)[1]\n",
    "        next_actions_t = torch.LongTensor(next_actions).reshape(-1,1).to(\n",
    "            device=self.network.device)\n",
    "        target_qvals = self.target_network.get_qvals(next_states)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_t).detach()\n",
    "        #################################################################\n",
    "        qvals_next[dones_t] = 0 # Zero-out terminal states\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_t\n",
    "        loss = nn.MSELoss()(qvals, expected_qvals)\n",
    "        return loss\n",
    "    \n",
    "    def update(self):\n",
    "        self.network.optimizer.zero_grad()\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
    "        loss = self.calculate_loss(batch)\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "        if self.network.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.training_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.update_loss = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.rewards = 0\n",
    "        self.step_count = 0\n",
    "        self.s_0 = self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "buffer = experienceReplayBuffer(memory_size=10000, burn_in=1000)\n",
    "ddqn = QNetwork(env, learning_rate=1e-3)\n",
    "agent = DDQNAgent(env, ddqn, buffer)\n",
    "agent.train(max_episodes=5000, network_update_frequency=4, \n",
    "            network_sync_frequency=1000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
