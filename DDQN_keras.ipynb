{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/Users/JeanJeongjinPark/opt/anaconda3/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b171378ab11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Double DQN for playing OpenAI Gym Environments. For full writeup, visit:\n",
    "# https://www.datahubbs.com/deep-q-learning-101/\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import namedtuple, deque, OrderedDict\n",
    "from copy import copy, deepcopy\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    args = parse_arguments()\n",
    "    if args.gpu is None or args.gpu == False:\n",
    "        args.gpu = 'cpu'\n",
    "    else:\n",
    "        args.gpu = 'cuda'\n",
    "\n",
    "    # Initialize environment\n",
    "    env = gym.make(args.env)\n",
    "\n",
    "    # Initialize DQNetwork\n",
    "    dqn = QNetwork(env=env, \n",
    "        n_hidden_layers=args.hl, \n",
    "        n_hidden_nodes=args.hn, \n",
    "        learning_rate=args.lr, \n",
    "        bias=args.bias,\n",
    "        tau=args.tau,\n",
    "        device=args.gpu)\n",
    "    # Initialize DQNAgent\n",
    "    agent = DQNAgent(env, dqn,  \n",
    "        memory_size=args.memorySize,\n",
    "        burn_in=args.burnIn,\n",
    "        reward_threshold=args.threshold,\n",
    "        path=args.path) \n",
    "    print(agent.network)\n",
    "    print(agent.target_network)\n",
    "    # Train agent\n",
    "    start_time = time.time()\n",
    "\n",
    "    agent.train(epsilon=args.epsStart,\n",
    "        gamma=args.gamma, \n",
    "        max_episodes=args.maxEps,\n",
    "        batch_size=args.batch,\n",
    "        update_freq = args.updateFreq,\n",
    "        network_sync_frequency=args.netSyncFreq)\n",
    "    end_time = time.time()\n",
    "    # Save results\n",
    "    if agent.success:\n",
    "        agent.save_results(args)\n",
    "        if args.plot:\n",
    "            agent.plot_rewards()\n",
    "    else:\n",
    "        shutil.rmtree(agent.path)\n",
    "\n",
    "    x = end_time - start_time\n",
    "    hours, remainder = divmod(x, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(\"Peak mean reward: {:.2f}\".format(\n",
    "        max(agent.mean_training_rewards)))\n",
    "    print(\"Training Time: {:02}:{:02}:{:02}\\n\".format(\n",
    "        int(hours), int(minutes), int(seconds)))\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, network, memory_size=50000,\n",
    "        batch_size=32, burn_in=10000, reward_threshold=None,\n",
    "        path=None, *args, **kwargs):\n",
    "        \n",
    "        self.env = env\n",
    "        self.env_name = env.spec.id\n",
    "        self.network = network\n",
    "        self.target_network = deepcopy(network)\n",
    "        self.tau = network.tau\n",
    "        self.batch_size = batch_size\n",
    "        self.window = 100\n",
    "        if reward_threshold is None:\n",
    "            self.reward_threshold = 195 if 'CartPole' in self.env_name \\\n",
    "                else 300\n",
    "        else:\n",
    "            self.reward_threshold = reward_threshold\n",
    "        self.path = path\n",
    "        self.timestamp = time.strftime('%Y%m%d_%H%M')\n",
    "        self.initialize(memory_size, burn_in)\n",
    "           \n",
    "    # Implement DQN training algorithm\n",
    "    def train(self, epsilon=0.05, gamma=0.99, max_episodes=10000,\n",
    "        batch_size=32, network_sync_frequency=5000, update_freq=4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        # Populate replay buffer\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            done = self.take_step(mode='explore')\n",
    "            if done:\n",
    "                self.s_0 = self.env.reset()\n",
    "            \n",
    "        ep = 0\n",
    "        training = True\n",
    "        while training:\n",
    "            self.s_0 = self.env.reset()\n",
    "            self.rewards = 0\n",
    "            done = False\n",
    "            while done == False:\n",
    "                done = self.take_step(mode='train')\n",
    "                # Update network\n",
    "                if self.step_count % update_freq == 0:\n",
    "                    self.update()\n",
    "                # Sync networks\n",
    "                if self.step_count % network_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.network.state_dict())\n",
    "                    \n",
    "                if done:\n",
    "                    ep += 1\n",
    "                    self.training_rewards.append(self.rewards)\n",
    "                    mean_rewards = np.mean(\n",
    "                        self.training_rewards[-self.window:])\n",
    "                    self.training_loss.append(np.mean(self.update_loss))\n",
    "                    self.update_loss = []\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                        ep, mean_rewards), end=\"\")\n",
    "                    \n",
    "                    if ep >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        self.success = True\n",
    "                        print('\\nEnvironment solved in {} steps!'.format(\n",
    "                            self.step_count))\n",
    "                        break\n",
    "\n",
    "    def take_step(self, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            s_0 = np.ravel(self.state_buffer)\n",
    "            action = self.network.get_action(s_0, epsilon=self.epsilon)\n",
    "            self.step_count += 1\n",
    "        s_1, r, done, _ = self.env.step(action)\n",
    "        self.rewards += r\n",
    "        self.state_buffer.append(self.s_0.copy())\n",
    "        self.next_state_buffer.append(s_1.copy())\n",
    "        self.buffer.append(deepcopy(self.state_buffer), action, r, done, \n",
    "                           deepcopy(self.next_state_buffer))\n",
    "        self.s_0 = s_1.copy()\n",
    "        return done\n",
    "\n",
    "    def calculate_loss(self, batch):\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_t = torch.FloatTensor(rewards).to(device=self.network.device).reshape(-1, 1)\n",
    "        actions_t = torch.LongTensor(np.array(actions)).to(\n",
    "            device=self.network.device).reshape(-1,1)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.network.device)\n",
    "        \n",
    "        #################################################################\n",
    "        # DDQN Update\n",
    "        next_actions = torch.max(self.network.get_qvals(next_states), dim=-1)[1]\n",
    "        next_actions_t = torch.LongTensor(next_actions).reshape(-1,1).to(\n",
    "            device=self.network.device)\n",
    "        target_qvals = self.target_network.get_qvals(next_states)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_t).detach()\n",
    "        #################################################################\n",
    "        qvals_next[dones_t] = 0 # Zero-out terminal states\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_t\n",
    "        loss = nn.MSELoss()(qvals, expected_qvals)\n",
    "        return loss\n",
    "    \n",
    "    def update(self):\n",
    "        self.network.optimizer.zero_grad()\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
    "        loss = self.calculate_loss(batch)\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "        if self.network.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "        \n",
    "    def initialize(self, memory_size, burn_in):\n",
    "        self.buffer = experienceReplayBuffer(memory_size, burn_in)\n",
    "        self.training_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.update_loss = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.rewards = 0\n",
    "        self.step_count = 0\n",
    "        self.s_0 = self.env.reset()\n",
    "        self.state_buffer = deque(maxlen=self.tau)\n",
    "        self.next_state_buffer = deque(maxlen=self.tau)\n",
    "        [self.state_buffer.append(np.zeros(self.s_0.size)) \n",
    "         for i in range(self.tau)]\n",
    "        [self.next_state_buffer.append(np.zeros(self.s_0.size)) \n",
    "         for i in range(self.tau)]\n",
    "        self.state_buffer.append(self.s_0)\n",
    "        self.success = False\n",
    "        if self.path is None:\n",
    "            self.path = os.path.join(os.getcwd(), \n",
    "                self.env_name, self.timestamp)\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(self.training_rewards, label='Rewards')\n",
    "        plt.plot(self.mean_training_rewards, label='Mean Rewards')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.ylim([0, np.round(self.reward_threshold)*1.05])\n",
    "        plt.savefig(os.path.join(self.path, 'rewards.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def save_results(self, args):\n",
    "        weights_path = os.path.join(self.path, 'dqn_weights.pt')\n",
    "        torch.save(self.network.state_dict(), weights_path)\n",
    "        # Save rewards\n",
    "        rewards = pd.DataFrame(self.training_rewards, columns=['reward'])\n",
    "        rewards.insert(0, 'episode', rewards.index.values)\n",
    "        rewards.to_csv(os.path.join(self.path, 'rewards.txt'))\n",
    "        # Save model parameters\n",
    "        file = open(os.path.join(self.path, 'parameters.txt'), 'w')\n",
    "        file.writelines('rewards')\n",
    "        [file.writelines('\\n' + str(k) + ',' + str(v)) \n",
    "            for k, v in vars(args).items()]\n",
    "        file.close()\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, n_hidden_layers=4,\n",
    "        n_hidden_nodes=256, bias=True, activation_function='relu',\n",
    "        tau=1, device='cpu', *args, **kwargs):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.tau = tau\n",
    "        n_inputs = env.observation_space.shape[0] * tau\n",
    "        self.n_inputs = n_inputs\n",
    "        n_outputs = env.action_space.n\n",
    "\n",
    "        activation_function = activation_function.lower()\n",
    "        if activation_function == 'relu':\n",
    "            act_func = nn.ReLU()\n",
    "        elif activation_function == 'tanh':\n",
    "            act_func = nn.Tanh()\n",
    "        elif activation_function == 'elu':\n",
    "            act_func = nn.ELU()\n",
    "        elif activation_function == 'sigmoid':\n",
    "            act_func = nn.Sigmoid()\n",
    "        elif activation_function == 'selu':\n",
    "            act_func = nn.SELU()\n",
    "        \n",
    "        # Build a network dependent on the hidden layer and node parameters\n",
    "        layers = OrderedDict()\n",
    "        n_layers = 2 * (n_hidden_layers-1)\n",
    "        for i in range(n_layers + 1):\n",
    "            if n_hidden_layers == 0:\n",
    "                layers[str(i)] = nn.Linear(\n",
    "                    n_inputs,\n",
    "                    n_outputs,\n",
    "                    bias=bias)\n",
    "            elif i == n_layers:\n",
    "                layers[str(i)] = nn.Linear(\n",
    "                    n_hidden_nodes,\n",
    "                    n_outputs,\n",
    "                    bias=bias)\n",
    "            elif i % 2 == 0 and i == 0:\n",
    "                layers[str(i)] = nn.Linear(\n",
    "                    n_inputs,\n",
    "                    n_hidden_nodes,\n",
    "                    bias=bias)\n",
    "            elif i % 2 == 0 and i < n_layers - 1:\n",
    "                layers[str(i)] = nn.Linear(\n",
    "                    n_hidden_nodes,\n",
    "                    n_hidden_nodes,\n",
    "                    bias=bias)\n",
    "            else:\n",
    "                layers[str(i)] = act_func\n",
    "                \n",
    "        self.network = nn.Sequential(layers)\n",
    "        \n",
    "        # Set device for GPU's\n",
    "        if self.device == 'cuda':\n",
    "            self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "                \n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            action = self.greedy_action(state)\n",
    "        return action\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        qvals = self.get_qvals(state)\n",
    "        return torch.max(qvals, dim=-1)[1].item()\n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.network(state_t)\n",
    "\n",
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.Buffer = namedtuple('Buffer', \n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, \n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque \n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.Buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in\n",
    "    \n",
    "    def capacity(self):\n",
    "        return len(self.replay_memory) / self.memory_size\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = ArgumentParser(description='Deep Q Network Argument Parser')\n",
    "    # Network parameters\n",
    "    parser.add_argument('--hl', type=int, default=3,\n",
    "        help='An integer number that defines the number of hidden layers.')\n",
    "    parser.add_argument('--hn', type=int, default=16,\n",
    "        help='An integer number that defines the number of hidden nodes.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "        help='An integer number that defines the number of hidden layers.')\n",
    "    parser.add_argument('--bias', type=str2bool, default=True,\n",
    "        help='Boolean to determine whether or not to use biases in network.')\n",
    "    parser.add_argument('--actFunc', type=str, default='relu',\n",
    "        help='Set activation function.')\n",
    "    parser.add_argument('--gpu', type=str2bool, default=False,\n",
    "        help='Boolean to enable GPU computation. Default set to False.')\n",
    "    # Environment\n",
    "    parser.add_argument('--env', dest='env', type=str, default='CartPole-v0')\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "        help='A value between 0 and 1 to discount future rewards.')\n",
    "    parser.add_argument('--maxEps', type=int, default=10000,\n",
    "        help='An integer number of episodes to train the agent on.')\n",
    "    parser.add_argument('--netSyncFreq', type=int, default=2000,\n",
    "        help='An integer number that defines steps to update the target network.')\n",
    "    parser.add_argument('--updateFreq', type=int, default=1,\n",
    "        help='Integer value that determines how many steps or episodes' + \n",
    "        'must be completed before a backpropogation update is taken.')\n",
    "    parser.add_argument('--batch', type=int, default=32,\n",
    "        help='An integer number that defines the batch size.')\n",
    "    parser.add_argument('--memorySize', type=int, default=50000,\n",
    "        help='An integer number that defines the replay buffer size.')\n",
    "    parser.add_argument('--burnIn', type=int, default=20000,\n",
    "        help='Set the number of random burn-in transitions before training.')\n",
    "    parser.add_argument('--epsStart', type=float, default=0.05,\n",
    "        help='Float value for the start of the epsilon decay.')\n",
    "    parser.add_argument('--epsEnd', type=float, default=0.01,\n",
    "        help='Float value for the end of the epsilon decay.')\n",
    "    parser.add_argument('--epsStrategy', type=str, default='constant',\n",
    "        help=\"Enter 'constant' to set epsilon to a constant value or 'decay'\" + \n",
    "        \"to have the value decay over time. If 'decay', ensure proper\" + \n",
    "        \"start and end values.\")\n",
    "    parser.add_argument('--tau', type=int, default=1,\n",
    "    \thelp='Number of states to link together.')\n",
    "    parser.add_argument('--epsConstant', type=float, default=0.05,\n",
    "        help='Float to be used in conjunction with a constant epsilon strategy.')\n",
    "    parser.add_argument('--window', type=int, default=100,\n",
    "        help='Integer value to set the moving average window.')\n",
    "    parser.add_argument('--plot', type=str2bool, default=True,\n",
    "        help='If true, plot training results.')\n",
    "    parser.add_argument('--path', type=str, default=None,\n",
    "        help='Specify path to save results.')\n",
    "    parser.add_argument('--threshold', type=int, default=195,\n",
    "        help='Set target reward threshold for the solved environment.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def str2bool(argument):\n",
    "    if argument.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif argument.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False \n",
    "    else:\n",
    "        raise ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "buffer = experienceReplayBuffer(memory_size=10000, burn_in=1000)\n",
    "ddqn = QNetwork(env, learning_rate=1e-3)\n",
    "agent = DDQNAgent(env, ddqn, buffer)\n",
    "agent.train(max_episodes=5000, network_update_frequency=4, \n",
    "            network_sync_frequency=1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d6fcabead560252fe7f65da1a5450be95ba7b5998027027be9479fa34aad379"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
